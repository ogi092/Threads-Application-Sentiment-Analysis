{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Model Inference\n",
    "\n",
    "---\n",
    "\n",
    "Gradded Challenge 7\n",
    "\n",
    "Name : Ogi Hadicahyo\n",
    "\n",
    "Batch  : HCK - 012 (Phase 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Importing Libraries and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# preprocess\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# model\n",
    "from tensorflow.keras.models import load_model\n",
    "# etc\n",
    "import joblib\n",
    "\n",
    "# Library for warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "INFO:tensorflow:Assets written to: best_model.H5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.H5\\assets\n"
     ]
    }
   ],
   "source": [
    "stopword_list= joblib.load('stopword_list.joblib')\n",
    "model = load_model('best_model.keras')\n",
    "model.save('best_model.H5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ogi\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.H5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()          # Define Lemmatizer\n",
    "    text = text.lower()                       # Lowercase text\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[^A-Za-z\\s']\", \"\", text)  # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
    "    tokens = word_tokenize(text)              # Tokenize\n",
    "    filtered_words = [word for word in tokens if word not in stopword_list]        # Stopwords Removal\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_words]        # Changing words to their basic form (lemmatization)\n",
    "    lemmatized_clean = [word.translate(str.maketrans('', '', string.punctuation)) for word in lemmatized_words] # Cleans the text from punctuation marks\n",
    "    return ' '.join(lemmatized_clean)\n",
    "\n",
    "def prediction(model, X):\n",
    "  y_pred = model.predict(X)\n",
    "  predictions = np.argmax(y_pred, axis=1)\n",
    "  for index, val in enumerate(predictions):\n",
    "    if val == 0:\n",
    "      print(f\"Text {index} indicates the person is giving positive sentiment\")\n",
    "    elif val == 1:\n",
    "      print(f\"Text {index} indicates the person is giving negative sentiment\")\n",
    "    else:\n",
    "      print(f\"Text {index} indicates the person is giving neutral sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input1= 'This app is so bad, I will never play it again'\n",
    "input2= 'I really enjoy this game, I think it is the best game I have ever played and I love it'\n",
    "input3= 'This application is normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "x = preprocess_text(input1)\n",
    "y = preprocess_text(input2)\n",
    "z = preprocess_text(input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Text 0 indicates the person is giving negative sentiment\n",
      "Text 1 indicates the person is giving positive sentiment\n",
      "Text 2 indicates the person is giving positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# predicting\n",
    "prediction(model, [x,y,z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the model works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
